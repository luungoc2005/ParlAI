!python -m parlai.scripts.train_distill_model \
-t fromfile:parlaiformat --fromfile_datapath "/content/drive/My Drive/reddit/train.txt" -dt train:stream \
-m transformer/generator_distilled \
-mf "/content/drive/My Drive/reddit/parlai_checkpoints/transformer-distilled-2" \
-teacher-model-file zoo:blender/blender_3B/model \
--dict-lower True \
--variant prelayernorm --n_positions=512 --truncate=512 \
--inference greedy --beam-size 1 --skip-generation True \
--share_word_embeddings True \
--embedding-size 768 --n-heads 12 --n-layers 12 --ffn_size 768 --activation relu --gradient-clip 0.1 \
--learn-positional-embeddings True --dropout 0.1 --attention-dropout 0.1 \
--num-epochs 1 -veps 0.01 -vme 512 -vmt ppl -vmm min -sval True \
-bs 32 --optimizer mem_eff_adam -lr 1e-05 \
--warmup_updates -1 --warmup_rate 0.0001 -vp 500 \
--fp16 True --fp16-impl mem_efficient --evaltask convai2